{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "\n",
      "The Last Line:  first to get up and stretch out her young body.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset: http://www.gutenberg.org/cache/epub/5200/pg5200.txt\n",
    "    Remove all the unnecessary data and label it as Metamorphosis-clean.\n",
    "    The starting and ending lines should be as follows.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "file = open(\"metamorphosis.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  3889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 17,  53],\n",
       "       [ 53, 293],\n",
       "       [293,   2],\n",
       "       [  2,  18],\n",
       "       [ 18, 729],\n",
       "       [729, 135],\n",
       "       [135, 730],\n",
       "       [730, 294],\n",
       "       [294,   8],\n",
       "       [  8, 731]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 17  53 293   2  18]\n",
      "The responses are:  [ 53 293   2  18 729]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 10)             26170     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1, 1000)           4044000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 1000)           8004000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1000)              8004000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2617)              2619617   \n",
      "=================================================================\n",
      "Total params: 23,698,787\n",
      "Trainable params: 23,698,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " 2/61 [..............................] - ETA: 1:09 - loss: 1.6379WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2128s vs `on_train_batch_end` time: 1.8573s). Check your callbacks.\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8949\n",
      "Epoch 00001: loss did not improve from 1.87881\n",
      "61/61 [==============================] - 16s 256ms/step - loss: 1.8949\n",
      "Epoch 2/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8409\n",
      "Epoch 00002: loss improved from 1.87881 to 1.84093, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 416ms/step - loss: 1.8409\n",
      "Epoch 3/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8422\n",
      "Epoch 00003: loss did not improve from 1.84093\n",
      "61/61 [==============================] - 14s 229ms/step - loss: 1.8422\n",
      "Epoch 4/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8407\n",
      "Epoch 00004: loss improved from 1.84093 to 1.84065, saving model to nextword1.h5\n",
      "61/61 [==============================] - 24s 394ms/step - loss: 1.8407\n",
      "Epoch 5/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7888\n",
      "Epoch 00005: loss improved from 1.84065 to 1.78883, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 325ms/step - loss: 1.7888\n",
      "Epoch 6/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7976\n",
      "Epoch 00006: loss did not improve from 1.78883\n",
      "61/61 [==============================] - 15s 240ms/step - loss: 1.7976\n",
      "Epoch 7/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7884\n",
      "Epoch 00007: loss improved from 1.78883 to 1.78838, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 443ms/step - loss: 1.7884\n",
      "Epoch 8/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7592\n",
      "Epoch 00008: loss improved from 1.78838 to 1.75918, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 318ms/step - loss: 1.7592\n",
      "Epoch 9/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7379\n",
      "Epoch 00009: loss improved from 1.75918 to 1.73787, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 354ms/step - loss: 1.7379\n",
      "Epoch 10/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7051\n",
      "Epoch 00010: loss improved from 1.73787 to 1.70511, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 1.7051\n",
      "Epoch 11/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6975\n",
      "Epoch 00011: loss improved from 1.70511 to 1.69745, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 328ms/step - loss: 1.6975\n",
      "Epoch 12/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6882\n",
      "Epoch 00012: loss improved from 1.69745 to 1.68818, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 1.6882\n",
      "Epoch 13/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6294\n",
      "Epoch 00013: loss improved from 1.68818 to 1.62936, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 366ms/step - loss: 1.6294\n",
      "Epoch 14/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5947\n",
      "Epoch 00014: loss improved from 1.62936 to 1.59467, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 319ms/step - loss: 1.5947\n",
      "Epoch 15/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6029\n",
      "Epoch 00015: loss did not improve from 1.59467\n",
      "61/61 [==============================] - 14s 222ms/step - loss: 1.6029\n",
      "Epoch 16/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6534\n",
      "Epoch 00016: loss did not improve from 1.59467\n",
      "61/61 [==============================] - 14s 222ms/step - loss: 1.6534\n",
      "Epoch 17/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5988\n",
      "Epoch 00017: loss did not improve from 1.59467\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "61/61 [==============================] - 14s 231ms/step - loss: 1.5988\n",
      "Epoch 18/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2100\n",
      "Epoch 00018: loss improved from 1.59467 to 1.21002, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 504ms/step - loss: 1.2100\n",
      "Epoch 19/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0711\n",
      "Epoch 00019: loss improved from 1.21002 to 1.07106, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 349ms/step - loss: 1.0711\n",
      "Epoch 20/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0124\n",
      "Epoch 00020: loss improved from 1.07106 to 1.01236, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 381ms/step - loss: 1.0124\n",
      "Epoch 21/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9823\n",
      "Epoch 00021: loss improved from 1.01236 to 0.98232, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.9823\n",
      "Epoch 22/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9619\n",
      "Epoch 00022: loss improved from 0.98232 to 0.96189, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 508ms/step - loss: 0.9619\n",
      "Epoch 23/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9460\n",
      "Epoch 00023: loss improved from 0.96189 to 0.94599, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 277ms/step - loss: 0.9460\n",
      "Epoch 24/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9344\n",
      "Epoch 00024: loss improved from 0.94599 to 0.93443, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 289ms/step - loss: 0.9344\n",
      "Epoch 25/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9235\n",
      "Epoch 00025: loss improved from 0.93443 to 0.92350, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 370ms/step - loss: 0.9235\n",
      "Epoch 26/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9158\n",
      "Epoch 00026: loss improved from 0.92350 to 0.91576, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 363ms/step - loss: 0.9158\n",
      "Epoch 27/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9076\n",
      "Epoch 00027: loss improved from 0.91576 to 0.90759, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 352ms/step - loss: 0.9076\n",
      "Epoch 28/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9004\n",
      "Epoch 00028: loss improved from 0.90759 to 0.90039, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.9004\n",
      "Epoch 29/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8967\n",
      "Epoch 00029: loss improved from 0.90039 to 0.89670, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.8967\n",
      "Epoch 30/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8887\n",
      "Epoch 00030: loss improved from 0.89670 to 0.88868, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 460ms/step - loss: 0.8887\n",
      "Epoch 31/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8866\n",
      "Epoch 00031: loss improved from 0.88868 to 0.88660, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 0.8866\n",
      "Epoch 32/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8798\n",
      "Epoch 00032: loss improved from 0.88660 to 0.87984, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 0.8798\n",
      "Epoch 33/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8766\n",
      "Epoch 00033: loss improved from 0.87984 to 0.87664, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 0.8766\n",
      "Epoch 34/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8720\n",
      "Epoch 00034: loss improved from 0.87664 to 0.87203, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 336ms/step - loss: 0.8720\n",
      "Epoch 35/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8706\n",
      "Epoch 00035: loss improved from 0.87203 to 0.87058, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 367ms/step - loss: 0.8706\n",
      "Epoch 36/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8654\n",
      "Epoch 00036: loss improved from 0.87058 to 0.86543, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 326ms/step - loss: 0.8654\n",
      "Epoch 37/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8614\n",
      "Epoch 00037: loss improved from 0.86543 to 0.86144, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 434ms/step - loss: 0.8614\n",
      "Epoch 38/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8593\n",
      "Epoch 00038: loss improved from 0.86144 to 0.85927, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 382ms/step - loss: 0.8593\n",
      "Epoch 39/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8545\n",
      "Epoch 00039: loss improved from 0.85927 to 0.85448, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 372ms/step - loss: 0.8545\n",
      "Epoch 40/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8534\n",
      "Epoch 00040: loss improved from 0.85448 to 0.85342, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 367ms/step - loss: 0.8534\n",
      "Epoch 41/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8462\n",
      "Epoch 00041: loss improved from 0.85342 to 0.84618, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 330ms/step - loss: 0.8462\n",
      "Epoch 42/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8444\n",
      "Epoch 00042: loss improved from 0.84618 to 0.84437, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 384ms/step - loss: 0.8444\n",
      "Epoch 43/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8409\n",
      "Epoch 00043: loss improved from 0.84437 to 0.84090, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 377ms/step - loss: 0.8409\n",
      "Epoch 44/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8423\n",
      "Epoch 00044: loss did not improve from 0.84090\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 0.8423\n",
      "Epoch 45/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8353\n",
      "Epoch 00045: loss improved from 0.84090 to 0.83530, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 374ms/step - loss: 0.8353\n",
      "Epoch 46/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8329\n",
      "Epoch 00046: loss improved from 0.83530 to 0.83291, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 319ms/step - loss: 0.8329\n",
      "Epoch 47/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8370\n",
      "Epoch 00047: loss did not improve from 0.83291\n",
      "61/61 [==============================] - 16s 256ms/step - loss: 0.8370\n",
      "Epoch 48/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8297\n",
      "Epoch 00048: loss improved from 0.83291 to 0.82973, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 277ms/step - loss: 0.8297\n",
      "Epoch 49/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8286\n",
      "Epoch 00049: loss improved from 0.82973 to 0.82858, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 347ms/step - loss: 0.8286\n",
      "Epoch 50/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8283\n",
      "Epoch 00050: loss improved from 0.82858 to 0.82831, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 291ms/step - loss: 0.8283\n",
      "Epoch 51/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8255\n",
      "Epoch 00051: loss improved from 0.82831 to 0.82550, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 280ms/step - loss: 0.8255\n",
      "Epoch 52/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8228\n",
      "Epoch 00052: loss improved from 0.82550 to 0.82280, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 281ms/step - loss: 0.8228\n",
      "Epoch 53/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8218\n",
      "Epoch 00053: loss improved from 0.82280 to 0.82182, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 0.8218\n",
      "Epoch 54/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8173\n",
      "Epoch 00054: loss improved from 0.82182 to 0.81725, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 294ms/step - loss: 0.8173\n",
      "Epoch 55/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8146\n",
      "Epoch 00055: loss improved from 0.81725 to 0.81455, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 290ms/step - loss: 0.8146\n",
      "Epoch 56/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8116\n",
      "Epoch 00056: loss improved from 0.81455 to 0.81161, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 289ms/step - loss: 0.8116\n",
      "Epoch 57/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8106\n",
      "Epoch 00057: loss improved from 0.81161 to 0.81064, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 323ms/step - loss: 0.8106\n",
      "Epoch 58/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8085\n",
      "Epoch 00058: loss improved from 0.81064 to 0.80850, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 292ms/step - loss: 0.8085\n",
      "Epoch 59/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8071\n",
      "Epoch 00059: loss improved from 0.80850 to 0.80712, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 283ms/step - loss: 0.8071\n",
      "Epoch 60/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8044\n",
      "Epoch 00060: loss improved from 0.80712 to 0.80440, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 280ms/step - loss: 0.8044\n",
      "Epoch 61/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8041\n",
      "Epoch 00061: loss improved from 0.80440 to 0.80413, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 279ms/step - loss: 0.8041\n",
      "Epoch 62/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8039\n",
      "Epoch 00062: loss improved from 0.80413 to 0.80391, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 285ms/step - loss: 0.8039\n",
      "Epoch 63/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7975\n",
      "Epoch 00063: loss improved from 0.80391 to 0.79752, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 274ms/step - loss: 0.7975\n",
      "Epoch 64/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7986\n",
      "Epoch 00064: loss did not improve from 0.79752\n",
      "61/61 [==============================] - 15s 243ms/step - loss: 0.7986\n",
      "Epoch 65/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7947\n",
      "Epoch 00065: loss improved from 0.79752 to 0.79466, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 292ms/step - loss: 0.7947\n",
      "Epoch 66/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7972\n",
      "Epoch 00066: loss did not improve from 0.79466\n",
      "61/61 [==============================] - 16s 255ms/step - loss: 0.7972\n",
      "Epoch 67/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7974\n",
      "Epoch 00067: loss did not improve from 0.79466\n",
      "61/61 [==============================] - 18s 302ms/step - loss: 0.7974\n",
      "Epoch 68/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7932\n",
      "Epoch 00068: loss improved from 0.79466 to 0.79325, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 348ms/step - loss: 0.7932\n",
      "Epoch 69/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7898\n",
      "Epoch 00069: loss improved from 0.79325 to 0.78979, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 290ms/step - loss: 0.7898\n",
      "Epoch 70/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7891\n",
      "Epoch 00070: loss improved from 0.78979 to 0.78915, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 347ms/step - loss: 0.7891\n",
      "Epoch 71/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7858\n",
      "Epoch 00071: loss improved from 0.78915 to 0.78585, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 0.7858\n",
      "Epoch 72/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7848\n",
      "Epoch 00072: loss improved from 0.78585 to 0.78476, saving model to nextword1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 17s 281ms/step - loss: 0.7848\n",
      "Epoch 73/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7852\n",
      "Epoch 00073: loss did not improve from 0.78476\n",
      "61/61 [==============================] - 15s 240ms/step - loss: 0.7852\n",
      "Epoch 74/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7810\n",
      "Epoch 00074: loss improved from 0.78476 to 0.78100, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 0.7810\n",
      "Epoch 75/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7782\n",
      "Epoch 00075: loss improved from 0.78100 to 0.77817, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 336ms/step - loss: 0.7782\n",
      "Epoch 76/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7780\n",
      "Epoch 00076: loss improved from 0.77817 to 0.77805, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 412ms/step - loss: 0.7780\n",
      "Epoch 77/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7759\n",
      "Epoch 00077: loss improved from 0.77805 to 0.77589, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 323ms/step - loss: 0.7759\n",
      "Epoch 78/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7774\n",
      "Epoch 00078: loss did not improve from 0.77589\n",
      "61/61 [==============================] - 14s 230ms/step - loss: 0.7774\n",
      "Epoch 79/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7782\n",
      "Epoch 00079: loss did not improve from 0.77589\n",
      "61/61 [==============================] - 14s 223ms/step - loss: 0.7782\n",
      "Epoch 80/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7734\n",
      "Epoch 00080: loss improved from 0.77589 to 0.77336, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 340ms/step - loss: 0.7734\n",
      "Epoch 81/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7706\n",
      "Epoch 00081: loss improved from 0.77336 to 0.77064, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 286ms/step - loss: 0.7706\n",
      "Epoch 82/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7675\n",
      "Epoch 00082: loss improved from 0.77064 to 0.76750, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 327ms/step - loss: 0.7675\n",
      "Epoch 83/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7716\n",
      "Epoch 00083: loss did not improve from 0.76750\n",
      "61/61 [==============================] - 14s 238ms/step - loss: 0.7716\n",
      "Epoch 84/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7710\n",
      "Epoch 00084: loss did not improve from 0.76750\n",
      "61/61 [==============================] - 15s 241ms/step - loss: 0.7710\n",
      "Epoch 85/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7653\n",
      "Epoch 00085: loss improved from 0.76750 to 0.76535, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 303ms/step - loss: 0.7653\n",
      "Epoch 86/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7643\n",
      "Epoch 00086: loss improved from 0.76535 to 0.76427, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 0.7643\n",
      "Epoch 87/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7607\n",
      "Epoch 00087: loss improved from 0.76427 to 0.76073, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 292ms/step - loss: 0.7607\n",
      "Epoch 88/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7624\n",
      "Epoch 00088: loss did not improve from 0.76073\n",
      "61/61 [==============================] - 14s 236ms/step - loss: 0.7624\n",
      "Epoch 89/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7619\n",
      "Epoch 00089: loss did not improve from 0.76073\n",
      "61/61 [==============================] - 17s 280ms/step - loss: 0.7619\n",
      "Epoch 90/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7609\n",
      "Epoch 00090: loss did not improve from 0.76073\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "61/61 [==============================] - 15s 246ms/step - loss: 0.7609\n",
      "Epoch 91/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7029\n",
      "Epoch 00091: loss improved from 0.76073 to 0.70293, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 322ms/step - loss: 0.7029\n",
      "Epoch 92/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6943\n",
      "Epoch 00092: loss improved from 0.70293 to 0.69430, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 342ms/step - loss: 0.6943\n",
      "Epoch 93/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6928\n",
      "Epoch 00093: loss improved from 0.69430 to 0.69284, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 320ms/step - loss: 0.6928\n",
      "Epoch 94/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6925\n",
      "Epoch 00094: loss improved from 0.69284 to 0.69248, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 289ms/step - loss: 0.6925\n",
      "Epoch 95/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6910\n",
      "Epoch 00095: loss improved from 0.69248 to 0.69102, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 271ms/step - loss: 0.6910\n",
      "Epoch 96/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6912\n",
      "Epoch 00096: loss did not improve from 0.69102\n",
      "61/61 [==============================] - 13s 215ms/step - loss: 0.6912\n",
      "Epoch 97/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6900\n",
      "Epoch 00097: loss improved from 0.69102 to 0.68998, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 260ms/step - loss: 0.6900\n",
      "Epoch 98/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6913\n",
      "Epoch 00098: loss did not improve from 0.68998\n",
      "61/61 [==============================] - 13s 211ms/step - loss: 0.6913\n",
      "Epoch 99/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6889\n",
      "Epoch 00099: loss improved from 0.68998 to 0.68893, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 252ms/step - loss: 0.6889\n",
      "Epoch 100/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6892\n",
      "Epoch 00100: loss did not improve from 0.68893\n",
      "61/61 [==============================] - 13s 211ms/step - loss: 0.6892\n",
      "Epoch 101/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6904\n",
      "Epoch 00101: loss did not improve from 0.68893\n",
      "61/61 [==============================] - 13s 213ms/step - loss: 0.6904\n",
      "Epoch 102/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6884\n",
      "Epoch 00102: loss improved from 0.68893 to 0.68839, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 254ms/step - loss: 0.6884\n",
      "Epoch 103/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6901\n",
      "Epoch 00103: loss did not improve from 0.68839\n",
      "61/61 [==============================] - 13s 206ms/step - loss: 0.6901\n",
      "Epoch 104/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6877\n",
      "Epoch 00104: loss improved from 0.68839 to 0.68766, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 261ms/step - loss: 0.6877\n",
      "Epoch 105/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6876\n",
      "Epoch 00105: loss improved from 0.68766 to 0.68760, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 260ms/step - loss: 0.6876\n",
      "Epoch 106/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6869\n",
      "Epoch 00106: loss improved from 0.68760 to 0.68687, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 253ms/step - loss: 0.6869\n",
      "Epoch 107/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6883\n",
      "Epoch 00107: loss did not improve from 0.68687\n",
      "61/61 [==============================] - 13s 212ms/step - loss: 0.6883\n",
      "Epoch 108/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6873\n",
      "Epoch 00108: loss did not improve from 0.68687\n",
      "61/61 [==============================] - 13s 215ms/step - loss: 0.6873\n",
      "Epoch 109/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6868\n",
      "Epoch 00109: loss improved from 0.68687 to 0.68684, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 255ms/step - loss: 0.6868\n",
      "Epoch 110/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6867\n",
      "Epoch 00110: loss improved from 0.68684 to 0.68673, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 260ms/step - loss: 0.6867\n",
      "Epoch 111/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6858\n",
      "Epoch 00111: loss improved from 0.68673 to 0.68584, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 266ms/step - loss: 0.6858\n",
      "Epoch 112/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6838\n",
      "Epoch 00112: loss improved from 0.68584 to 0.68382, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 261ms/step - loss: 0.6838\n",
      "Epoch 113/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6848\n",
      "Epoch 00113: loss did not improve from 0.68382\n",
      "61/61 [==============================] - 13s 214ms/step - loss: 0.6848\n",
      "Epoch 114/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6843\n",
      "Epoch 00114: loss did not improve from 0.68382\n",
      "61/61 [==============================] - 13s 212ms/step - loss: 0.6843\n",
      "Epoch 115/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6839\n",
      "Epoch 00115: loss did not improve from 0.68382\n",
      "61/61 [==============================] - 13s 209ms/step - loss: 0.6839\n",
      "Epoch 116/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6830\n",
      "Epoch 00116: loss improved from 0.68382 to 0.68297, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 323ms/step - loss: 0.6830\n",
      "Epoch 117/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6834\n",
      "Epoch 00117: loss did not improve from 0.68297\n",
      "61/61 [==============================] - 13s 213ms/step - loss: 0.6834\n",
      "Epoch 118/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6821\n",
      "Epoch 00118: loss improved from 0.68297 to 0.68211, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 270ms/step - loss: 0.6821\n",
      "Epoch 119/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6819\n",
      "Epoch 00119: loss improved from 0.68211 to 0.68188, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 245ms/step - loss: 0.6819\n",
      "Epoch 120/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6829\n",
      "Epoch 00120: loss did not improve from 0.68188\n",
      "61/61 [==============================] - 13s 208ms/step - loss: 0.6829\n",
      "Epoch 121/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6792\n",
      "Epoch 00121: loss improved from 0.68188 to 0.67922, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 268ms/step - loss: 0.6792\n",
      "Epoch 122/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6819\n",
      "Epoch 00122: loss did not improve from 0.67922\n",
      "61/61 [==============================] - 13s 212ms/step - loss: 0.6819\n",
      "Epoch 123/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6805\n",
      "Epoch 00123: loss did not improve from 0.67922\n",
      "61/61 [==============================] - 13s 211ms/step - loss: 0.6805\n",
      "Epoch 124/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6831\n",
      "Epoch 00124: loss did not improve from 0.67922\n",
      "61/61 [==============================] - 13s 213ms/step - loss: 0.6831\n",
      "Epoch 125/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6795\n",
      "Epoch 00125: loss did not improve from 0.67922\n",
      "61/61 [==============================] - 13s 211ms/step - loss: 0.6795\n",
      "Epoch 126/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6792\n",
      "Epoch 00126: loss improved from 0.67922 to 0.67921, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 239ms/step - loss: 0.6792\n",
      "Epoch 127/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6800\n",
      "Epoch 00127: loss did not improve from 0.67921\n",
      "61/61 [==============================] - 13s 208ms/step - loss: 0.6800\n",
      "Epoch 128/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6794\n",
      "Epoch 00128: loss did not improve from 0.67921\n",
      "61/61 [==============================] - 13s 214ms/step - loss: 0.6794\n",
      "Epoch 129/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6790\n",
      "Epoch 00129: loss improved from 0.67921 to 0.67897, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 262ms/step - loss: 0.6790\n",
      "Epoch 130/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6787\n",
      "Epoch 00130: loss improved from 0.67897 to 0.67874, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 274ms/step - loss: 0.6787\n",
      "Epoch 131/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6798\n",
      "Epoch 00131: loss did not improve from 0.67874\n",
      "61/61 [==============================] - 13s 218ms/step - loss: 0.6798\n",
      "Epoch 132/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6778\n",
      "Epoch 00132: loss improved from 0.67874 to 0.67780, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 0.6778\n",
      "Epoch 133/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.676 - ETA: 0s - loss: 0.6770\n",
      "Epoch 00133: loss improved from 0.67780 to 0.67699, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 264ms/step - loss: 0.6770\n",
      "Epoch 134/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6768\n",
      "Epoch 00134: loss improved from 0.67699 to 0.67676, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 269ms/step - loss: 0.6768\n",
      "Epoch 135/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6764- ETA: 2s - l\n",
      "Epoch 00135: loss improved from 0.67676 to 0.67637, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 284ms/step - loss: 0.6764\n",
      "Epoch 136/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6768\n",
      "Epoch 00136: loss did not improve from 0.67637\n",
      "61/61 [==============================] - 14s 222ms/step - loss: 0.6768\n",
      "Epoch 137/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6765\n",
      "Epoch 00137: loss did not improve from 0.67637\n",
      "61/61 [==============================] - 13s 218ms/step - loss: 0.6765\n",
      "Epoch 138/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6750\n",
      "Epoch 00138: loss improved from 0.67637 to 0.67502, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 242ms/step - loss: 0.6750\n",
      "Epoch 139/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6744\n",
      "Epoch 00139: loss improved from 0.67502 to 0.67439, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 254ms/step - loss: 0.6744\n",
      "Epoch 140/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6733\n",
      "Epoch 00140: loss improved from 0.67439 to 0.67335, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 250ms/step - loss: 0.6733\n",
      "Epoch 141/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6727\n",
      "Epoch 00141: loss improved from 0.67335 to 0.67268, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 253ms/step - loss: 0.6727\n",
      "Epoch 142/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6736\n",
      "Epoch 00142: loss did not improve from 0.67268\n",
      "61/61 [==============================] - 13s 209ms/step - loss: 0.6736\n",
      "Epoch 143/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6715\n",
      "Epoch 00143: loss improved from 0.67268 to 0.67146, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 243ms/step - loss: 0.6715\n",
      "Epoch 144/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6722\n",
      "Epoch 00144: loss did not improve from 0.67146\n",
      "61/61 [==============================] - 14s 223ms/step - loss: 0.6722\n",
      "Epoch 145/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6733\n",
      "Epoch 00145: loss did not improve from 0.67146\n",
      "61/61 [==============================] - 14s 230ms/step - loss: 0.6733\n",
      "Epoch 146/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6731\n",
      "Epoch 00146: loss did not improve from 0.67146\n",
      "61/61 [==============================] - 13s 211ms/step - loss: 0.6731\n",
      "Epoch 147/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6712\n",
      "Epoch 00147: loss improved from 0.67146 to 0.67117, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 264ms/step - loss: 0.6712\n",
      "Epoch 148/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - ETA: 0s - loss: 0.6731\n",
      "Epoch 00148: loss did not improve from 0.67117\n",
      "61/61 [==============================] - 13s 209ms/step - loss: 0.6731\n",
      "Epoch 149/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6708\n",
      "Epoch 00149: loss improved from 0.67117 to 0.67082, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 241ms/step - loss: 0.6708\n",
      "Epoch 150/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6720\n",
      "Epoch 00150: loss did not improve from 0.67082\n",
      "61/61 [==============================] - 13s 216ms/step - loss: 0.6720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x296286d0730>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
